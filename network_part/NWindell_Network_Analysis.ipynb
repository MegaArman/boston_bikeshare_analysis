{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101444c4-28dc-490a-b4bc-2c01c443f253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4702180, 6)\n",
      "\n",
      "Columns: ['start_station_id', 'end_station_id', 'started_at', 'start_station_name', 'end_station_name', 'tripduration']\n",
      "\n",
      "Dtypes:\n",
      " start_station_id              object\n",
      "end_station_id                object\n",
      "started_at            datetime64[ns]\n",
      "start_station_name            object\n",
      "end_station_name              object\n",
      "tripduration                 float64\n",
      "dtype: object\n",
      "\n",
      "Head:\n",
      "   start_station_id end_station_id              started_at  \\\n",
      "0           M32085         S32023 2024-08-01 19:57:49.371   \n",
      "1           K32017         D32035 2024-08-01 19:57:49.672   \n",
      "2           C32077         C32005 2024-08-01 19:57:49.879   \n",
      "\n",
      "             start_station_name            end_station_name  tripduration  \n",
      "0     Mass Ave/Lafayette Square                  30 Dane St           NaN  \n",
      "1     Harvard St and Stedman St  Harvard Ave at Brainerd Rd           NaN  \n",
      "2  Columbus Ave at W. Canton St   Washington St at Lenox St           NaN  \n",
      "\n",
      "Missingness (%):\n",
      "tripduration          100.0\n",
      "start_station_id        0.0\n",
      "end_station_id          0.0\n",
      "started_at              0.0\n",
      "start_station_name      0.0\n",
      "end_station_name        0.0\n",
      "dtype: float64\n",
      "\n",
      "Blank start_station_id (%): 0.0000\n",
      "Blank end_station_id   (%): 0.0000\n",
      "\n",
      "Unparseable started_at (%): 0.0000\n",
      "\n",
      "Exact duplicate rows (%): 0.0004\n",
      "\n",
      "Self-loop trips (%): 3.2518\n",
      "\n",
      "Unique counts:\n",
      "  Unique start stations: 572\n",
      "  Unique end stations  : 572\n",
      "  Unique stations total: 572\n",
      "\n",
      "Time range:\n",
      "  min: 2024-07-31 16:48:26.662000\n",
      "  max: 2025-07-31 23:57:11.774000\n",
      "\n",
      "start_station_id -> multiple start_station_name mappings (% of IDs): 2.4476\n",
      "\n",
      "end_station_id -> multiple end_station_name mappings (% of IDs): 2.4476\n",
      "\n",
      "tripduration non-null (%): 0.0000\n",
      "\n",
      "INTEGRITY VERDICT: OK ✅\n"
     ]
    }
   ],
   "source": [
    "#DATA INTEGRITY CHECK\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load (fastparquet) ---\n",
    "path = \"network_data.parquet\"\n",
    "df = pd.read_parquet(path, engine=\"fastparquet\")\n",
    "\n",
    "# --- Basic snapshot ---\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "print(\"\\nDtypes:\\n\", df.dtypes)\n",
    "print(\"\\nHead:\\n\", df.head(3))\n",
    "\n",
    "# --- Required columns check ---\n",
    "required = {\"start_station_id\", \"end_station_id\", \"started_at\"}\n",
    "missing = sorted(required - set(df.columns))\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# --- Null / missingness ---\n",
    "print(\"\\nMissingness (%):\")\n",
    "print((df.isna().mean() * 100).sort_values(ascending=False).round(3))\n",
    "\n",
    "# --- Basic validity checks ---\n",
    "# 1) Empty / whitespace station IDs\n",
    "def _blank(x):\n",
    "    return x.isna() | (x.astype(str).str.strip() == \"\")\n",
    "\n",
    "blank_start = _blank(df[\"start_station_id\"]).mean()\n",
    "blank_end   = _blank(df[\"end_station_id\"]).mean()\n",
    "print(f\"\\nBlank start_station_id (%): {blank_start*100:.4f}\")\n",
    "print(f\"Blank end_station_id   (%): {blank_end*100:.4f}\")\n",
    "\n",
    "# 2) Ensure started_at is datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(df[\"started_at\"]):\n",
    "    df[\"started_at\"] = pd.to_datetime(df[\"started_at\"], errors=\"coerce\", utc=False)\n",
    "\n",
    "bad_time = df[\"started_at\"].isna().mean()\n",
    "print(f\"\\nUnparseable started_at (%): {bad_time*100:.4f}\")\n",
    "\n",
    "# 3) Duplicates (exact row duplicates)\n",
    "dup_rate = df.duplicated().mean()\n",
    "print(f\"\\nExact duplicate rows (%): {dup_rate*100:.4f}\")\n",
    "\n",
    "# 4) Self-loops (start == end)\n",
    "self_loop_rate = (df[\"start_station_id\"].astype(str) == df[\"end_station_id\"].astype(str)).mean()\n",
    "print(f\"\\nSelf-loop trips (%): {self_loop_rate*100:.4f}\")\n",
    "\n",
    "# --- Cardinalities / coverage ---\n",
    "n_start = df[\"start_station_id\"].nunique(dropna=True)\n",
    "n_end   = df[\"end_station_id\"].nunique(dropna=True)\n",
    "n_nodes = pd.Index(df[\"start_station_id\"].dropna().astype(str)).union(\n",
    "          pd.Index(df[\"end_station_id\"].dropna().astype(str))).nunique()\n",
    "\n",
    "print(\"\\nUnique counts:\")\n",
    "print(\"  Unique start stations:\", n_start)\n",
    "print(\"  Unique end stations  :\", n_end)\n",
    "print(\"  Unique stations total:\", n_nodes)\n",
    "\n",
    "# --- Time range & basic distribution ---\n",
    "tmin, tmax = df[\"started_at\"].min(), df[\"started_at\"].max()\n",
    "print(\"\\nTime range:\")\n",
    "print(\"  min:\", tmin)\n",
    "print(\"  max:\", tmax)\n",
    "\n",
    "# --- Station name consistency (optional, if columns exist) ---\n",
    "# Checks whether an ID maps to multiple names (data hygiene)\n",
    "for id_col, name_col in [\n",
    "    (\"start_station_id\", \"start_station_name\"),\n",
    "    (\"end_station_id\", \"end_station_name\"),\n",
    "]:\n",
    "    if id_col in df.columns and name_col in df.columns:\n",
    "        tmp = df[[id_col, name_col]].dropna()\n",
    "        multi_name = (tmp.groupby(tmp[id_col].astype(str))[name_col].nunique() > 1).mean()\n",
    "        print(f\"\\n{ id_col } -> multiple { name_col } mappings (% of IDs): {multi_name*100:.4f}\")\n",
    "\n",
    "# --- Tripduration sanity (optional) ---\n",
    "if \"tripduration\" in df.columns:\n",
    "    non_null = df[\"tripduration\"].notna().mean()\n",
    "    print(f\"\\ntripduration non-null (%): {non_null*100:.4f}\")\n",
    "    if non_null > 0:\n",
    "        # show a few stats if it's populated\n",
    "        td = pd.to_numeric(df[\"tripduration\"], errors=\"coerce\")\n",
    "        print(\"tripduration stats (sec):\")\n",
    "        print(td.describe(percentiles=[0.01, 0.5, 0.99]).round(3))\n",
    "\n",
    "issues = []\n",
    "if blank_start > 0: issues.append(\"blank start_station_id\")\n",
    "if blank_end > 0: issues.append(\"blank end_station_id\")\n",
    "if bad_time > 0: issues.append(\"unparseable started_at\")\n",
    "\n",
    "print(\"\\nINTEGRITY VERDICT:\", \"OK ✅\" if not issues else f\"CHECK ⚠️ ({', '.join(issues)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f7a619-0578-4d27-bde3-862c102a79ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved: network_metrics_all.csv, network_metrics_<season>.csv, top20_consensus_*.csv, top20_stability_jaccard.csv\n",
      "\n",
      "Top-20 stability matrix (Jaccard overlap, rounded to 3 decimals):\n",
      "         Winter  Spring  Summer   Fall\n",
      "Winter   1.000   0.667   0.481  0.538\n",
      "Spring   0.667   1.000   0.739  0.667\n",
      "Summer   0.481   0.739   1.000  0.667\n",
      "Fall     0.538   0.667   0.667  1.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import warnings\n",
    "# Ignore all warnings (global)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "PATH = \"network_data.parquet\"\n",
    "df = pd.read_parquet(PATH, engine=\"fastparquet\")\n",
    "\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Ensure datetime\n",
    "df[\"started_at\"] = pd.to_datetime(df[\"started_at\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"start_station_id\", \"end_station_id\", \"started_at\"])\n",
    "\n",
    "# Hour bin for aggregation (kept for future extensions, not strictly needed for season graphs)\n",
    "df[\"hour\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "\n",
    "# Season label (meteorological seasons)\n",
    "def season_from_month(m: int) -> str:\n",
    "    if m in (12, 1, 2):  return \"Winter\"\n",
    "    if m in (3, 4, 5):   return \"Spring\"\n",
    "    if m in (6, 7, 8):   return \"Summer\"\n",
    "    return \"Fall\"\n",
    "\n",
    "df[\"season\"] = df[\"started_at\"].dt.month.map(season_from_month)\n",
    "\n",
    "start_lookup = (\n",
    "    df[[\"start_station_id\", \"start_station_name\"]]\n",
    "    .dropna()\n",
    "    .groupby(\"start_station_id\")[\"start_station_name\"]\n",
    "    .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0])\n",
    ")\n",
    "end_lookup = (\n",
    "    df[[\"end_station_id\", \"end_station_name\"]]\n",
    "    .dropna()\n",
    "    .groupby(\"end_station_id\")[\"end_station_name\"]\n",
    "    .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0])\n",
    ")\n",
    "\n",
    "# One station-name dict for labeling\n",
    "name_lookup = start_lookup.combine_first(end_lookup).to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build edge lists (overall + by season)\n",
    "# ----------------------------\n",
    "def make_edges(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Weighted directed edges: count trips as weight\n",
    "    edges = (\n",
    "        frame.groupby([\"start_station_id\", \"end_station_id\"])\n",
    "             .size()\n",
    "             .reset_index(name=\"weight\")\n",
    "    )\n",
    "    return edges\n",
    "\n",
    "edges_all = make_edges(df)\n",
    "edges_by_season = {s: make_edges(df[df[\"season\"] == s]) for s in [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]}\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Graph build + centralities\n",
    "# ----------------------------\n",
    "def build_graph(edges: pd.DataFrame) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for u, v, w in edges.itertuples(index=False):\n",
    "        G.add_edge(u, v, weight=float(w))\n",
    "    return G\n",
    "\n",
    "def centralities(G: nx.DiGraph, drop_self_loops_for_betweenness: bool = True) -> pd.DataFrame:\n",
    "    # Strength = weighted degree\n",
    "    out_strength = dict(G.out_degree(weight=\"weight\"))\n",
    "    in_strength  = dict(G.in_degree(weight=\"weight\"))\n",
    "\n",
    "    # Weighted PageRank\n",
    "    pr = nx.pagerank(G, weight=\"weight\")\n",
    "\n",
    "    # Betweenness: interpret \"cost\" as inverse of flow weight (stronger flow = shorter distance)\n",
    "    H = G.copy()\n",
    "    if drop_self_loops_for_betweenness:\n",
    "        H.remove_edges_from(nx.selfloop_edges(H))\n",
    "\n",
    "    for u, v, d in H.edges(data=True):\n",
    "        w = d.get(\"weight\", 1.0)\n",
    "        d[\"distance\"] = 1.0 / (w + 1e-12)\n",
    "\n",
    "    btw = nx.betweenness_centrality(H, weight=\"distance\", normalized=True)\n",
    "\n",
    "    # Combine\n",
    "    nodes = sorted(set(G.nodes()))\n",
    "    out_s = pd.Series(out_strength, name=\"out_strength\").reindex(nodes).fillna(0.0)\n",
    "    in_s  = pd.Series(in_strength,  name=\"in_strength\").reindex(nodes).fillna(0.0)\n",
    "    pr_s  = pd.Series(pr,           name=\"pagerank\").reindex(nodes).fillna(0.0)\n",
    "    btw_s = pd.Series(btw,          name=\"betweenness\").reindex(nodes).fillna(0.0)\n",
    "\n",
    "    out = pd.concat([out_s, in_s, pr_s, btw_s], axis=1).reset_index(names=\"station_id\")\n",
    "    out[\"station_name\"] = out[\"station_id\"].map(name_lookup)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Consensus top-k via rank aggregation\n",
    "# ----------------------------\n",
    "def add_consensus_rank(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = metrics.copy()\n",
    "\n",
    "    # Higher is better → rank descending\n",
    "    for col in [\"out_strength\", \"betweenness\", \"pagerank\"]:\n",
    "        m[f\"rank_{col}\"] = m[col].rank(ascending=False, method=\"average\")\n",
    "\n",
    "    # Borda-style: lower total rank = better\n",
    "    m[\"consensus_rank_sum\"] = m[[\"rank_out_strength\", \"rank_betweenness\", \"rank_pagerank\"]].sum(axis=1)\n",
    "    m[\"consensus_rank\"] = m[\"consensus_rank_sum\"].rank(ascending=True, method=\"dense\")\n",
    "\n",
    "    # Handy plotting score\n",
    "    m[\"consensus_score\"] = 1.0 / (m[\"consensus_rank_sum\"] + 1e-12)\n",
    "\n",
    "    return m.sort_values([\"consensus_rank_sum\"]).reset_index(drop=True)\n",
    "\n",
    "# Compute metrics\n",
    "G_all = build_graph(edges_all)\n",
    "metrics_all = add_consensus_rank(centralities(G_all))\n",
    "\n",
    "metrics_season = {}\n",
    "for s, e in edges_by_season.items():\n",
    "    Gs = build_graph(e)\n",
    "    metrics_season[s] = add_consensus_rank(centralities(Gs))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Top-20 lists + stability (Jaccard overlap)\n",
    "# ----------------------------\n",
    "def top_k_ids(metrics: pd.DataFrame, k: int = 20, by: str = \"consensus_rank_sum\"):\n",
    "    return list(metrics.sort_values(by).head(k)[\"station_id\"])\n",
    "\n",
    "def jaccard(a, b) -> float:\n",
    "    A, B = set(a), set(b)\n",
    "    return len(A & B) / max(1, len(A | B))\n",
    "\n",
    "seasons = [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]\n",
    "top20_by_season = {s: top_k_ids(metrics_season[s], 20) for s in seasons}\n",
    "\n",
    "stab = pd.DataFrame(index=seasons, columns=seasons, dtype=float)\n",
    "for i in seasons:\n",
    "    for j in seasons:\n",
    "        stab.loc[i, j] = jaccard(top20_by_season[i], top20_by_season[j])\n",
    "\n",
    "def round_numeric(df_in: pd.DataFrame, decimals: int = 3) -> pd.DataFrame:\n",
    "    out = df_in.copy()\n",
    "    num_cols = out.select_dtypes(include=[np.number]).columns\n",
    "    out[num_cols] = out[num_cols].round(decimals)\n",
    "    return out\n",
    "\n",
    "metrics_all_r = round_numeric(metrics_all, 3)\n",
    "metrics_season_r = {s: round_numeric(m, 3) for s, m in metrics_season.items()}\n",
    "stab_r = stab.round(3)\n",
    "\n",
    "metrics_all_r.to_csv(\"network_metrics_all.csv\", index=False)\n",
    "for s in seasons:\n",
    "    metrics_season_r[s].to_csv(f\"network_metrics_{s.lower()}.csv\", index=False)\n",
    "\n",
    "stab_r.to_csv(\"top20_stability_jaccard.csv\")\n",
    "\n",
    "top20_all = metrics_all_r.sort_values(\"consensus_rank_sum\").head(20)\n",
    "top20_all.to_csv(\"top20_consensus_all.csv\", index=False)\n",
    "\n",
    "for s in seasons:\n",
    "    top20 = metrics_season_r[s].sort_values(\"consensus_rank_sum\").head(20)\n",
    "    top20.to_csv(f\"top20_consensus_{s.lower()}.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Saved: network_metrics_all.csv, network_metrics_<season>.csv, top20_consensus_*.csv, top20_stability_jaccard.csv\")\n",
    "print(\"\\nTop-20 stability matrix (Jaccard overlap, rounded to 3 decimals):\\n\", stab_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38c801a-bb6e-468a-ac80-7b0643a7edb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact normalized name match rate: 95.98%\n",
      "\n",
      "Example unmatched stations (first 10):\n",
      "station_id                     station_name\n",
      "      <NA>                             <NA>\n",
      "    A32003    B.U. Central - 725 Comm. Ave.\n",
      "    A32037     Washington St at Egremont Rd\n",
      "    A32043     Western Ave at Richardson St\n",
      "    B32017 Dudley Square - Bolling Building\n",
      "    B32060            700 Commonwealth Ave.\n",
      "    BCBS01                     BCBS Hingham\n",
      "    C32049       Thetford Ave at Norfolk St\n",
      "    C32063               Mass Ave T Station\n",
      "    C32065          Adams St at Lonsdale St\n",
      "\n",
      "Saved interactive map: boston_bluebikes_network_map.html\n",
      "Stations plotted: 549 / 572 (95.98%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import folium\n",
    "\n",
    "# --- Load your parquet (fastparquet) ---\n",
    "PATH = \"network_data.parquet\"\n",
    "df = pd.read_parquet(PATH, engine=\"fastparquet\").drop_duplicates()\n",
    "\n",
    "df[\"start_station_id\"] = df[\"start_station_id\"].astype(str).str.strip()\n",
    "df[\"end_station_id\"]   = df[\"end_station_id\"].astype(str).str.strip()\n",
    "\n",
    "# Build canonical names from your parquet (mode name per station_id)\n",
    "start_lookup = (\n",
    "    df[[\"start_station_id\", \"start_station_name\"]]\n",
    "    .dropna()\n",
    "    .groupby(\"start_station_id\")[\"start_station_name\"]\n",
    "    .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0])\n",
    ")\n",
    "end_lookup = (\n",
    "    df[[\"end_station_id\", \"end_station_name\"]]\n",
    "    .dropna()\n",
    "    .groupby(\"end_station_id\")[\"end_station_name\"]\n",
    "    .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0])\n",
    ")\n",
    "\n",
    "name_lookup = start_lookup.combine_first(end_lookup).to_dict()\n",
    "\n",
    "metrics_all_r = pd.read_csv(\"network_metrics_all.csv\")\n",
    "metrics_all_r[\"station_id\"] = metrics_all_r[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "# Ensure station_name exists\n",
    "if \"station_name\" not in metrics_all_r.columns:\n",
    "    metrics_all_r[\"station_name\"] = metrics_all_r[\"station_id\"].map(name_lookup)\n",
    "\n",
    "GBFS_DISCOVERY = \"https://gbfs.bluebikes.com/gbfs/gbfs.json\"\n",
    "gbfs = requests.get(GBFS_DISCOVERY, timeout=30).json()\n",
    "feeds = gbfs[\"data\"][\"en\"][\"feeds\"]\n",
    "station_info_url = next(f[\"url\"] for f in feeds if f[\"name\"] == \"station_information\")\n",
    "\n",
    "station_info = requests.get(station_info_url, timeout=30).json()\n",
    "gbfs_st = pd.DataFrame(station_info[\"data\"][\"stations\"])\n",
    "\n",
    "gbfs_st = gbfs_st.rename(columns={\"name\": \"gbfs_station_name\"}).copy()\n",
    "gbfs_st[\"gbfs_station_name\"] = gbfs_st[\"gbfs_station_name\"].astype(str)\n",
    "\n",
    "gbfs_st[\"lat\"] = pd.to_numeric(gbfs_st[\"lat\"], errors=\"coerce\")\n",
    "gbfs_st[\"lon\"] = pd.to_numeric(gbfs_st[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)     # remove punctuation\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()    # collapse whitespace\n",
    "    # common cleanups (optional)\n",
    "    s = s.replace(\" at \", \" \")            # \"X at Y\" vs \"X/Y\" style drift\n",
    "    return s\n",
    "\n",
    "# normalized tables\n",
    "parquet_names = (\n",
    "    pd.Series(name_lookup, name=\"station_name\")\n",
    "      .reset_index()\n",
    "      .rename(columns={\"index\": \"station_id\"})\n",
    ")\n",
    "parquet_names[\"norm\"] = parquet_names[\"station_name\"].astype(str).map(norm_name)\n",
    "\n",
    "gbfs_st[\"norm\"] = gbfs_st[\"gbfs_station_name\"].map(norm_name)\n",
    "\n",
    "# Use exact normalized match first\n",
    "gbfs_by_norm = gbfs_st.dropna(subset=[\"lat\",\"lon\"]).drop_duplicates(\"norm\").set_index(\"norm\")[[\"gbfs_station_name\",\"lat\",\"lon\"]]\n",
    "parquet_names = parquet_names.join(gbfs_by_norm, on=\"norm\", how=\"left\")\n",
    "\n",
    "# Report match rate\n",
    "match_rate = parquet_names[\"lat\"].notna().mean() * 100\n",
    "print(f\"Exact normalized name match rate: {match_rate:.2f}%\")\n",
    "\n",
    "unmatched = parquet_names[parquet_names[\"lat\"].isna()][[\"station_id\",\"station_name\"]].head(10)\n",
    "if len(unmatched):\n",
    "    print(\"\\nExample unmatched stations (first 10):\")\n",
    "    print(unmatched.to_string(index=False))\n",
    "\n",
    "# Build station_id -> lat/lon mapping\n",
    "id_to_latlon = parquet_names.set_index(\"station_id\")[[\"lat\",\"lon\",\"gbfs_station_name\"]]\n",
    "\n",
    "metrics_geo = metrics_all_r.merge(\n",
    "    id_to_latlon.reset_index(),\n",
    "    on=\"station_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "plot_df = metrics_geo.dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "if plot_df.empty:\n",
    "    raise ValueError(\n",
    "        \"No stations matched to lat/lon. \"\n",
    "        \"This suggests station names differ too much; we can add fuzzy matching.\"\n",
    "    )\n",
    "\n",
    "center_lat, center_lon = float(plot_df[\"lat\"].mean()), float(plot_df[\"lon\"].mean())\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Marker radius based on consensus_score (log scaled)\n",
    "cs = pd.to_numeric(plot_df[\"consensus_score\"], errors=\"coerce\").fillna(0).values\n",
    "radii = (np.log1p(cs * 1e6) + 1) * 2.5\n",
    "\n",
    "for (_, row), r in zip(plot_df.iterrows(), radii):\n",
    "    nm = row.get(\"gbfs_station_name\") if pd.notna(row.get(\"gbfs_station_name\")) else row.get(\"station_name\")\n",
    "    tooltip = (\n",
    "        f\"{nm}<br>\"\n",
    "        f\"station_id={row['station_id']}<br>\"\n",
    "        f\"consensus_score={float(row['consensus_score']):.3f}<br>\"\n",
    "        f\"pagerank={float(row['pagerank']):.3f}, betweenness={float(row['betweenness']):.3f}<br>\"\n",
    "        f\"out_strength={float(row['out_strength']):.3f}, in_strength={float(row['in_strength']):.3f}\"\n",
    "    )\n",
    "    folium.CircleMarker(\n",
    "        location=[float(row[\"lat\"]), float(row[\"lon\"])],\n",
    "        radius=float(r),\n",
    "        fill=True,\n",
    "        fill_opacity=0.70,\n",
    "        opacity=0.80,\n",
    "        tooltip=folium.Tooltip(tooltip, sticky=True),\n",
    "    ).add_to(m)\n",
    "\n",
    "OUT_HTML = \"boston_bluebikes_network_map.html\"\n",
    "m.save(OUT_HTML)\n",
    "print(f\"\\nSaved interactive map: {OUT_HTML}\")\n",
    "print(f\"Stations plotted: {len(plot_df)} / {len(metrics_geo)} ({len(plot_df)/len(metrics_geo)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e56b36-21e0-4e9b-b2ab-667f00ea1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: boston_network_topN_markers.html (Top 120 stations)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import folium\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "TOP_N = 120          \n",
    "MIN_R = 3\n",
    "MAX_R = 14\n",
    "\n",
    "# Keep only top-N by importance\n",
    "plot_top = plot_df.sort_values(\"consensus_score\", ascending=False).head(TOP_N).copy()\n",
    "\n",
    "# Robust scaling: quantiles -> [MIN_R, MAX_R]\n",
    "scores = plot_top[\"consensus_score\"].astype(float).values\n",
    "lo, hi = np.quantile(scores, 0.05), np.quantile(scores, 0.95)\n",
    "scaled = (np.clip(scores, lo, hi) - lo) / (hi - lo + 1e-12)\n",
    "radii = MIN_R + scaled * (MAX_R - MIN_R)\n",
    "\n",
    "# Center map\n",
    "center_lat, center_lon = float(plot_top[\"lat\"].mean()), float(plot_top[\"lon\"].mean())\n",
    "m1 = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "for (_, row), r in zip(plot_top.iterrows(), radii):\n",
    "    nm = row.get(\"gbfs_station_name\") if \"gbfs_station_name\" in row and not pd.isna(row.get(\"gbfs_station_name\")) else row.get(\"station_name\")\n",
    "    tooltip = (\n",
    "        f\"{nm}<br>\"\n",
    "        f\"consensus_score={float(row['consensus_score']):.3f}<br>\"\n",
    "        f\"pagerank={float(row['pagerank']):.3f}, betweenness={float(row['betweenness']):.3f}<br>\"\n",
    "        f\"out_strength={float(row['out_strength']):.3f}, in_strength={float(row['in_strength']):.3f}\"\n",
    "    )\n",
    "\n",
    "    # \"Nicer\" marker: circle + border and a bit more contrast\n",
    "    folium.CircleMarker(\n",
    "        location=[float(row[\"lat\"]), float(row[\"lon\"])],\n",
    "        radius=float(r),\n",
    "        weight=2,            # border thickness\n",
    "        fill=True,\n",
    "        fill_opacity=0.75,\n",
    "        opacity=0.9,\n",
    "        tooltip=folium.Tooltip(tooltip, sticky=True),\n",
    "    ).add_to(m1)\n",
    "\n",
    "OUT1 = \"boston_network_topN_markers.html\"\n",
    "m1.save(OUT1)\n",
    "print(f\"Saved: {OUT1} (Top {TOP_N} stations)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc88accb-b686-48a0-a3d9-18b58b9324d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: boston_network_heatmap.html (Top 250 stations)\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import numpy as np\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "TOP_N = 250          \n",
    "WEIGHT_POWER = 0.5   # <1 compresses extremes; >1 emphasizes top hubs\n",
    "RADIUS = 18\n",
    "BLUR = 22\n",
    "MAX_ZOOM = 13\n",
    "\n",
    "plot_heat = plot_df.sort_values(\"consensus_score\", ascending=False).head(TOP_N).copy()\n",
    "\n",
    "# Create heat weights from consensus_score\n",
    "w = plot_heat[\"consensus_score\"].astype(float).values\n",
    "w = np.power(w / (w.max() + 1e-12), WEIGHT_POWER)  # normalize then shape distribution\n",
    "\n",
    "heat_data = list(zip(plot_heat[\"lat\"].astype(float), plot_heat[\"lon\"].astype(float), w))\n",
    "\n",
    "center_lat, center_lon = float(plot_heat[\"lat\"].mean()), float(plot_heat[\"lon\"].mean())\n",
    "m2 = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "HeatMap(\n",
    "    heat_data,\n",
    "    radius=RADIUS,\n",
    "    blur=BLUR,\n",
    "    max_zoom=MAX_ZOOM,\n",
    ").add_to(m2)\n",
    "\n",
    "OUT2 = \"boston_network_heatmap.html\"\n",
    "m2.save(OUT2)\n",
    "print(f\"Saved: {OUT2} (Top {TOP_N} stations)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e80b0c6e-1bc2-449b-ab93-3875f225fec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: boston_heat_importance.html (heatmap of consensus_score, top_n=300)\n",
      "Saved: boston_heat_demand.html (heatmap of out_strength, top_n=300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# plot_df must contain lat, lon, consensus_score, out_strength\n",
    "# If out_strength is huge, we normalize; Folium heatmap expects relative weights.\n",
    "\n",
    "def make_heatmap(df_in, weight_col, out_html,\n",
    "                 top_n=300, weight_power=0.5, radius=18, blur=22, max_zoom=13):\n",
    "    d = df_in.dropna(subset=[\"lat\", \"lon\", weight_col]).copy()\n",
    "    d = d.sort_values(weight_col, ascending=False).head(top_n)\n",
    "\n",
    "    w = d[weight_col].astype(float).values\n",
    "    # normalize then shape distribution so it isn't dominated by a few stations\n",
    "    w = np.power(w / (w.max() + 1e-12), weight_power)\n",
    "\n",
    "    heat_data = list(zip(d[\"lat\"].astype(float), d[\"lon\"].astype(float), w))\n",
    "\n",
    "    center_lat, center_lon = float(d[\"lat\"].mean()), float(d[\"lon\"].mean())\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "    HeatMap(heat_data, radius=radius, blur=blur, max_zoom=max_zoom).add_to(m)\n",
    "    m.save(out_html)\n",
    "    print(f\"Saved: {out_html} (heatmap of {weight_col}, top_n={top_n})\")\n",
    "\n",
    "# 1) Importance density (consensus_score)\n",
    "make_heatmap(\n",
    "    plot_df,\n",
    "    weight_col=\"consensus_score\",\n",
    "    out_html=\"boston_heat_importance.html\",\n",
    "    top_n=300,\n",
    "    weight_power=0.5\n",
    ")\n",
    "\n",
    "# 2) Demand density (out_strength)\n",
    "make_heatmap(\n",
    "    plot_df,\n",
    "    weight_col=\"out_strength\",\n",
    "    out_html=\"boston_heat_demand.html\",\n",
    "    top_n=300,\n",
    "    weight_power=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca12a500-d461-4fb4-81fc-f5dc8085e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: boston_markers_importance.html (marker size = consensus_score, top_n=120)\n",
      "Saved: boston_markers_demand.html (marker size = out_strength, top_n=120)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "def marker_map(df_in, weight_col, out_html,\n",
    "               top_n=120, min_r=3, max_r=14):\n",
    "    d = df_in.dropna(subset=[\"lat\", \"lon\", weight_col]).copy()\n",
    "    d = d.sort_values(weight_col, ascending=False).head(top_n)\n",
    "\n",
    "    # quantile scaling so sizes spread nicely\n",
    "    w = d[weight_col].astype(float).values\n",
    "    lo, hi = np.quantile(w, 0.05), np.quantile(w, 0.95)\n",
    "    scaled = (np.clip(w, lo, hi) - lo) / (hi - lo + 1e-12)\n",
    "    radii = min_r + scaled * (max_r - min_r)\n",
    "\n",
    "    center_lat, center_lon = float(d[\"lat\"].mean()), float(d[\"lon\"].mean())\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "    for (_, row), r in zip(d.iterrows(), radii):\n",
    "        nm = row.get(\"gbfs_station_name\")\n",
    "        if pd.isna(nm) or nm is None:\n",
    "            nm = row.get(\"station_name\", row.get(\"station_id\"))\n",
    "\n",
    "        tooltip = (\n",
    "            f\"{nm}<br>\"\n",
    "            f\"{weight_col}={float(row[weight_col]):.3f}<br>\"\n",
    "            f\"pagerank={float(row.get('pagerank', 0)):.3f}, betweenness={float(row.get('betweenness', 0)):.3f}<br>\"\n",
    "            f\"out_strength={float(row.get('out_strength', 0)):.3f}, in_strength={float(row.get('in_strength', 0)):.3f}\"\n",
    "        )\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            location=[float(row[\"lat\"]), float(row[\"lon\"])],\n",
    "            radius=float(r),\n",
    "            weight=2,            # border thickness\n",
    "            fill=True,\n",
    "            fill_opacity=0.75,\n",
    "            opacity=0.9,\n",
    "            tooltip=folium.Tooltip(tooltip, sticky=True),\n",
    "        ).add_to(m)\n",
    "\n",
    "    m.save(out_html)\n",
    "    print(f\"Saved: {out_html} (marker size = {weight_col}, top_n={top_n})\")\n",
    "\n",
    "# --- Importance map ---\n",
    "marker_map(\n",
    "    plot_df,\n",
    "    weight_col=\"consensus_score\",\n",
    "    out_html=\"boston_markers_importance.html\",\n",
    "    top_n=120\n",
    ")\n",
    "\n",
    "# --- Demand map ---\n",
    "marker_map(\n",
    "    plot_df,\n",
    "    weight_col=\"out_strength\",\n",
    "    out_html=\"boston_markers_demand.html\",\n",
    "    top_n=120\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f46d1515-cb8f-4dd3-9107-82de820cce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: boston_markers_importance_emphasized.html  | size~consensus_score (gamma=2.6), color~consensus_score bins, top_n=120\n",
      "Saved: boston_markers_demand_emphasized.html  | size~out_strength (gamma=1.8), color~out_strength bins, top_n=120\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "def add_legend(m, title, labels, colors):\n",
    "    # Simple HTML legend box\n",
    "    items = \"\".join(\n",
    "        f\"<div style='display:flex; align-items:center; margin:2px 0;'>\"\n",
    "        f\"<span style='width:14px; height:14px; background:{c}; display:inline-block; \"\n",
    "        f\"margin-right:8px; border:1px solid #666;'></span>\"\n",
    "        f\"<span style='font-size:12px;'>{lab}</span></div>\"\n",
    "        for lab, c in zip(labels, colors)\n",
    "    )\n",
    "    html = f\"\"\"\n",
    "    <div style=\"\n",
    "        position: fixed; bottom: 25px; left: 25px; z-index: 9999;\n",
    "        background: rgba(255,255,255,0.92); padding: 10px 12px;\n",
    "        border: 1px solid #999; border-radius: 8px; box-shadow: 0 1px 8px rgba(0,0,0,0.2);\n",
    "        \">\n",
    "        <div style=\"font-size:13px; font-weight:600; margin-bottom:6px;\">{title}</div>\n",
    "        {items}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(html))\n",
    "\n",
    "def marker_map_emphasized(\n",
    "    df_in,\n",
    "    weight_col,\n",
    "    out_html,\n",
    "    top_n=120,\n",
    "    min_r=3,\n",
    "    max_r=22,\n",
    "    gamma=2.0,         # >1 emphasizes top stations more\n",
    "    n_bins=5,          # quantile bins for color\n",
    "    show_edges=False\n",
    "):\n",
    "    d = df_in.dropna(subset=[\"lat\", \"lon\", weight_col]).copy()\n",
    "    d[weight_col] = pd.to_numeric(d[weight_col], errors=\"coerce\")\n",
    "    d = d.dropna(subset=[weight_col])\n",
    "\n",
    "    d = d.sort_values(weight_col, ascending=False).head(top_n).copy()\n",
    "\n",
    "    # ----- Size scaling -----\n",
    "    w = d[weight_col].astype(float).values\n",
    "    # robust range\n",
    "    lo, hi = np.quantile(w, 0.05), np.quantile(w, 0.95)\n",
    "    x = (np.clip(w, lo, hi) - lo) / (hi - lo + 1e-12)  # [0,1]\n",
    "    x = np.power(x, gamma)                              # emphasize top\n",
    "    radii = min_r + x * (max_r - min_r)\n",
    "\n",
    "    # ----- Color bins (quantiles) -----\n",
    "    # Create bin edges from quantiles of the selected top_n\n",
    "    qs = np.quantile(w, np.linspace(0, 1, n_bins + 1))\n",
    "    # ensure strictly increasing edges (rare ties can cause duplicates)\n",
    "    qs = np.unique(qs)\n",
    "    if len(qs) < 3:\n",
    "        # fallback if values are too tied\n",
    "        qs = np.array([w.min(), np.median(w), w.max()])\n",
    "\n",
    "    # Define a simple light->dark palette (you can change these)\n",
    "    colors = [\"#cfe8ff\", \"#9ecae1\", \"#6baed6\", \"#3182bd\", \"#08519c\"]\n",
    "    colors = colors[-(len(qs)-1):]  # match number of intervals\n",
    "\n",
    "    def color_for(val):\n",
    "        # find interval index\n",
    "        idx = np.searchsorted(qs, val, side=\"right\") - 1\n",
    "        idx = max(0, min(idx, len(qs) - 2))\n",
    "        return colors[idx]\n",
    "\n",
    "    # Legend labels: show ranges\n",
    "    labels = []\n",
    "    for i in range(len(qs)-1):\n",
    "        labels.append(f\"{qs[i]:.3f} – {qs[i+1]:.3f}\")\n",
    "\n",
    "    # ----- Build map -----\n",
    "    center_lat, center_lon = float(d[\"lat\"].mean()), float(d[\"lon\"].mean())\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "    for (_, row), r in zip(d.iterrows(), radii):\n",
    "        nm = row.get(\"gbfs_station_name\")\n",
    "        if pd.isna(nm) or nm is None:\n",
    "            nm = row.get(\"station_name\", row.get(\"station_id\"))\n",
    "\n",
    "        val = float(row[weight_col])\n",
    "        tooltip = (\n",
    "            f\"{nm}<br>\"\n",
    "            f\"{weight_col}={val:.3f}<br>\"\n",
    "            f\"pagerank={float(row.get('pagerank', 0)):.3f}, betweenness={float(row.get('betweenness', 0)):.3f}<br>\"\n",
    "            f\"out_strength={float(row.get('out_strength', 0)):.3f}, in_strength={float(row.get('in_strength', 0)):.3f}\"\n",
    "        )\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            location=[float(row[\"lat\"]), float(row[\"lon\"])],\n",
    "            radius=float(r),\n",
    "            color=\"#333333\",\n",
    "            weight=1,\n",
    "            fill=True,\n",
    "            fill_color=color_for(val),\n",
    "            fill_opacity=0.78,\n",
    "            opacity=0.9,\n",
    "            tooltip=folium.Tooltip(tooltip, sticky=True),\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add legend\n",
    "    add_legend(\n",
    "        m,\n",
    "        title=f\"{weight_col} bins (Top {top_n})\",\n",
    "        labels=labels,\n",
    "        colors=colors\n",
    "    )\n",
    "\n",
    "    m.save(out_html)\n",
    "    print(f\"Saved: {out_html}  | size~{weight_col} (gamma={gamma}), color~{weight_col} bins, top_n={top_n}\")\n",
    "\n",
    "# --- Importance (structural centrality) map ---\n",
    "marker_map_emphasized(\n",
    "    plot_df,\n",
    "    weight_col=\"consensus_score\",\n",
    "    out_html=\"boston_markers_importance_emphasized.html\",\n",
    "    top_n=120,\n",
    "    max_r=24,\n",
    "    gamma=2.6,   # stronger emphasis helps because consensus_score is compressed\n",
    "    n_bins=5\n",
    ")\n",
    "\n",
    "# --- Demand (volume) map ---\n",
    "marker_map_emphasized(\n",
    "    plot_df,\n",
    "    weight_col=\"out_strength\",\n",
    "    out_html=\"boston_markers_demand_emphasized.html\",\n",
    "    top_n=120,\n",
    "    max_r=24,\n",
    "    gamma=1.8,   # demand usually already has more spread\n",
    "    n_bins=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502b7bca-696a-4eb0-807f-a444414c961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: boston_markers_importance_emphasized.html  | size~consensus_score (gamma=2.6), color~consensus_score bins, top_n=120\n",
      "Saved: boston_markers_demand_emphasized.html  | size~out_strength (gamma=1.8), color~out_strength bins, top_n=120\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "def add_legend(m, title, labels, colors):\n",
    "    items = \"\".join(\n",
    "        f\"<div style='display:flex; align-items:center; margin:6px 0;'>\"\n",
    "        f\"<span style='width:20px; height:20px; background:{c}; display:inline-block; \"\n",
    "        f\"margin-right:12px; border:1px solid #666;'></span>\"\n",
    "        f\"<span style='font-size:16px; line-height:1.2;'>{lab}</span></div>\"\n",
    "        for lab, c in zip(labels, colors)\n",
    "    )\n",
    "    html = f\"\"\"\n",
    "    <div style=\"\n",
    "        position: fixed; bottom: 25px; left: 25px; z-index: 9999;\n",
    "        width: 300px;\n",
    "        background: rgba(255,255,255,0.94); padding: 16px 18px;\n",
    "        border: 1px solid #999; border-radius: 12px;\n",
    "        box-shadow: 0 2px 10px rgba(0,0,0,0.25);\n",
    "        \">\n",
    "        <div style=\"font-size:18px; font-weight:700; margin-bottom:12px;\">{title}</div>\n",
    "        {items}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(html))\n",
    "\n",
    "def marker_map_emphasized(\n",
    "    df_in,\n",
    "    weight_col,\n",
    "    out_html,\n",
    "    top_n=120,\n",
    "    min_r=3,\n",
    "    max_r=22,\n",
    "    gamma=2.0,         # >1 emphasizes top stations more\n",
    "    n_bins=5,          # quantile bins for color\n",
    "    show_edges=False\n",
    "):\n",
    "    d = df_in.dropna(subset=[\"lat\", \"lon\", weight_col]).copy()\n",
    "    d[weight_col] = pd.to_numeric(d[weight_col], errors=\"coerce\")\n",
    "    d = d.dropna(subset=[weight_col])\n",
    "\n",
    "    d = d.sort_values(weight_col, ascending=False).head(top_n).copy()\n",
    "\n",
    "    # ----- Size scaling -----\n",
    "    w = d[weight_col].astype(float).values\n",
    "    # robust range\n",
    "    lo, hi = np.quantile(w, 0.05), np.quantile(w, 0.95)\n",
    "    x = (np.clip(w, lo, hi) - lo) / (hi - lo + 1e-12)  # [0,1]\n",
    "    x = np.power(x, gamma)                              # emphasize top\n",
    "    radii = min_r + x * (max_r - min_r)\n",
    "\n",
    "    # ----- Color bins (quantiles) -----\n",
    "    qs = np.quantile(w, np.linspace(0, 1, n_bins + 1))\n",
    "    qs = np.unique(qs)\n",
    "    if len(qs) < 3:\n",
    "        qs = np.array([w.min(), np.median(w), w.max()])\n",
    "\n",
    "    colors = [\"#cfe8ff\", \"#9ecae1\", \"#6baed6\", \"#3182bd\", \"#08519c\"]\n",
    "    colors = colors[-(len(qs)-1):]\n",
    "\n",
    "    def color_for(val):\n",
    "        idx = np.searchsorted(qs, val, side=\"right\") - 1\n",
    "        idx = max(0, min(idx, len(qs) - 2))\n",
    "        return colors[idx]\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(qs)-1):\n",
    "        labels.append(f\"{qs[i]:.3f} – {qs[i+1]:.3f}\")\n",
    "\n",
    "    # ----- Build map -----\n",
    "    center_lat, center_lon = float(d[\"lat\"].mean()), float(d[\"lon\"].mean())\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "    for (_, row), r in zip(d.iterrows(), radii):\n",
    "        nm = row.get(\"gbfs_station_name\")\n",
    "        if pd.isna(nm) or nm is None:\n",
    "            nm = row.get(\"station_name\", row.get(\"station_id\"))\n",
    "\n",
    "        val = float(row[weight_col])\n",
    "        tooltip = (\n",
    "            f\"{nm}<br>\"\n",
    "            f\"{weight_col}={val:.3f}<br>\"\n",
    "            f\"pagerank={float(row.get('pagerank', 0)):.3f}, betweenness={float(row.get('betweenness', 0)):.3f}<br>\"\n",
    "            f\"out_strength={float(row.get('out_strength', 0)):.3f}, in_strength={float(row.get('in_strength', 0)):.3f}\"\n",
    "        )\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            location=[float(row[\"lat\"]), float(row[\"lon\"])],\n",
    "            radius=float(r),\n",
    "            color=\"#333333\",\n",
    "            weight=1,\n",
    "            fill=True,\n",
    "            fill_color=color_for(val),\n",
    "            fill_opacity=0.78,\n",
    "            opacity=0.9,\n",
    "            tooltip=folium.Tooltip(tooltip, sticky=True),\n",
    "        ).add_to(m)\n",
    "\n",
    "    add_legend(\n",
    "        m,\n",
    "        title=f\"{weight_col} bins (Top {top_n})\",\n",
    "        labels=labels,\n",
    "        colors=colors\n",
    "    )\n",
    "\n",
    "    m.save(out_html)\n",
    "    print(f\"Saved: {out_html}  | size~{weight_col} (gamma={gamma}), color~{weight_col} bins, top_n={top_n}\")\n",
    "\n",
    "# --- Importance (structural centrality) map ---\n",
    "marker_map_emphasized(\n",
    "    plot_df,\n",
    "    weight_col=\"consensus_score\",\n",
    "    out_html=\"boston_markers_importance_emphasized.html\",\n",
    "    top_n=120,\n",
    "    max_r=24,\n",
    "    gamma=2.6,\n",
    "    n_bins=5\n",
    ")\n",
    "\n",
    "# --- Demand (volume) map ---\n",
    "marker_map_emphasized(\n",
    "    plot_df,\n",
    "    weight_col=\"out_strength\",\n",
    "    out_html=\"boston_markers_demand_emphasized.html\",\n",
    "    top_n=120,\n",
    "    max_r=24,\n",
    "    gamma=1.8,\n",
    "    n_bins=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa5deb-8255-44e1-b549-92190b3368cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
